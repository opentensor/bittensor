{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Model Inference\n",
    "\n",
    "Welcome! This notebook is a tutorial on how to use the model you've just trained on the Bittensor network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import bittensor\n",
    "from nuclei.gpt2 import GPT2Nucleus\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained model\n",
    "You can find the model under `~/.bittensor/miners/gpt2-exodus/<wallet-coldkey>-<wallet-hotkey>/model.torch`. This is the default place that miners will store models. Note that the loss stored with the model is the `combined` loss, that is, the local loss + remote network loss + distillation loss. As the Bittensor network grows and more sophisticated models join, this combined loss will come down close to 0 ideally. \n",
    "\n",
    "However, for now, the high loss does not necessarily mean the model will perform badly as a model may have a low local loss but a high remote loss. This happens when the local model is powerful and correctly training, but all the models it is talking to on the network are not so good. This can happen when it's talking to N number of models that are all the same. Since this project is still in the early days, this may happen initially. As the network grows there will be more and more sophisticated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined loss (local, remote, and distilled) of preloaded model: inf:\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.expanduser('~/.bittensor/miners/default-default/gpt2_exodus/')\n",
    "\n",
    "# Check which device this machine is on, just in case we're not loading the model on the same machine that we trained it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load(\"{}/model.torch\".format(model_path), map_location=device)\n",
    "\n",
    "# Let's load up a Bittensor config\n",
    "config = GPT2Nucleus.config()\n",
    "\n",
    "# Let's load up the same nucleus config we trained our model with\n",
    "config.nucleus.n_head = 32\n",
    "config.nucleus.n_layer = 12\n",
    "config.nucleus.block_size = 20\n",
    "config.nucleus.device = device\n",
    "\n",
    "# Load up the model\n",
    "model = GPT2Nucleus(config)\n",
    "model.load_state_dict(checkpoint['nucleus_state'])\n",
    "print(\"Combined loss (local, remote, and distilled) of preloaded model: {}:\".format(checkpoint['epoch_loss']))\n",
    "# Load up the huggingface tokenizer\n",
    "tokenizer = bittensor.tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference function\n",
    "In essence, the output of the current GPT model is simply encoded using the HuggingFace tokenizer that Bittensor uses. We need to simply decode that information out using the same tokenizer and turn it into text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
    "    \"\"\"\n",
    "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
    "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
    "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
    "    of block_size, unlike an RNN that has an infinite context window.\n",
    "    \"\"\"\n",
    "    block_size = model.get_block_size()-1\n",
    "    model.eval()\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        \n",
    "        # Run a local forward call through the model\n",
    "        logits = model.local_forward(x_cond, training=False)\n",
    "        \n",
    "        # The final layer from the local forward (the local hidden layer) needs to be pushed\n",
    "        # through the target layer. This helps push the dimensionality to bittensor.__vocab_size__\n",
    "        # making it possible to push this information through the tokenizer's decode function to get\n",
    "        # words out.\n",
    "        logits = model.target_layer(logits.local_hidden)\n",
    "        \n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        \n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the trained model\n",
    "\n",
    "Now that we've got our `sample` function built, let's actually use it! We start our sentence using the `context` variable by giving it a name, and we let the model do the rest. Note that we can actually ask the model to predict whatever number of words we want. In this case, we made it 10 words as that produces legible sentences. The lower your model's loss is, the better predictions you'll get. \n",
    "\n",
    "Bring it as close to 0 as you can by changing up the `nucleus` parameters to adjust the model's architecture (number of heads, number of layers, etc.) or you can change up the training settings by changing the `miner` settings (things like learning rate, weight decay rate, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John is believed to have kissed Mary.John believes Bill\n"
     ]
    }
   ],
   "source": [
    "context = \"John\"\n",
    "\n",
    "# Tokenize the input\n",
    "x = tokenizer(context, padding=True, truncation=True)['input_ids']\n",
    "# Turn it into a tensor\n",
    "x = torch.tensor(x, dtype=torch.long)\n",
    "# Give it an extra dimension for the network's sake (expects a 2D tensor input)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "num_words_predict = 10\n",
    "\n",
    "# Let's sample the network for some output\n",
    "y = sample(model, x, num_words_predict, temperature=1.0, sample=True, top_k=10)\n",
    "\n",
    "# Decode the output\n",
    "completion = ''.join([tokenizer.decode(i, skip_special_tokens=True) for i in y])\n",
    "\n",
    "# Print what the model has predicted\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
