
// python3 -m grpc.tools.protoc bittensor/_proto/text_seq2seq/messages.proto  -I. --python_out=. --grpc_python_out=. --proto_path bittensor/_proto/

syntax = "proto3";
import "tensors/messages.proto";

message ForwardTextSeq2SeqResponse {
    int32 version = 1;
	string hotkey = 2;	
	Tensor generation_text = 5;
}

message ForwardTextSeq2SeqRequest {

    int32 version = 1;
	string hotkey = 2;	

	Serializer prompt_text_serializer = 3;
	Serializer generation_text_serializer = 4;

	Tensor prompt_text = 5;

    // Specifies the number of Topk logits to return.
    // Logit values are packed (pos, value) acocording 
    // to the bittensor tokenizer vocab size.
    int32 topk = 6;

    // Number of tokens to predict
    int32 num_to_generate = 7;

    //Generate Arguments
    // Number of beams
    int32 num_beams = 8;

    //Number of repeat words
    int32 no_repeat_ngram_size = 9;

    //Early Stopping
    bool early_stopping = 10;
    
    //Number of return seuqences
    int32 num_return_sequences = 11;

    //If sampling should be used
    bool do_sample = 12;

    //The probability cutoff
    float top_p = 13;

    //temperature of the softmax function
    float temperature = 15;

    //penalty for repeated words
    float repetition_penalty = 16; 

    //penalty for length 
    float length_penalty = 17;

    //maximum amount of time 
    float max_time = 18;

    //groups for beam search
    int32 num_beam_groups = 19;
}